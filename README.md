# 뉴스데이터로 주가지수 예측하는 논문 재현하기

나는 개인적으로 주식과 주가데이터에 관심이 많다. <br>
<br>
이 글은 내가 예전에 [뉴스와 주가 빅데이터 감성분석을 통한 지능형 의사결정모형](http://www.dbpia.co.kr/Journal/ArticleDetail/NODE01901732) 이라는 논문을 읽고 직접 재현해본 기록을 담은 글이다. <br>
<br>
결론부터 말하자면, 실패했다(내 경우엔 실패했다. 다른 재야의 고수들은 어떨지 모르겠다). <br>
이 글은 그 과정을 기록한 글이다. <br>
<br>
<br>

### Motive

해당 논문을 간단하게 요약하면 이렇다. <br>
<br>
1. 경제관련 인터넷 뉴스는 그날 주가지수의 등락과 연관성이 있을것이다.<br>
2. 경제관련 인터넷 뉴스를 형태소 분석기에 집어넣어서 해당 뉴스에 쓰인 단어들의 빈도를 뽑아낸다.<br>
3. 주가지수가 오른날의 뉴스기사에 쓰인 단어들엔 점수를 주고, 내린날에 쓰인 단어들엔 점수를 깎는다. 이를 '단어의 감성을 분석한다' 라고 부른다.<br>
4. 이런 방식으로 오랜 기간동안의 뉴스 데이터를 분석한다면 주가지수의 등락과 높은 연관성을 지닌 단어들을 발견할 수 있을것이다.<br>
<br>
나의 실험도 위와 같은 목차로 진행된다.<br>
<br>
<br>

### 뉴스 수집

뉴스 데이터 수집엔 파이썬의 [Scrapy][https://scrapy.org/] 라는 라이브러리를 사용했다. <br>

수집 대상으로 결정한 뉴스 포털은 네이버 금융의 [뉴스포커스 - 시황 및 전망](https://finance.naver.com/news/news_list.nhn?mode=LSS3D&section_id=101&section_id2=258&section_id3=401) 게시판이다.

![naver_financial_news]()

2005년 1월 1일부터 2017년 12월 1일까지 대략 13년간의 뉴스 데이터를 긁어왔다.

'시황 및 전망' 카테고리 말고도 다른 카테고리에서도 긁어오려고 했으나, 다른 카테고리의 경우 

단순히 현재 어떤 수치에 대한 정보만 알려주고 끝나는 경우도 있고(이런 경우는 기사의 내용도 한줄 또는 두줄밖에 안되는 경우가 많아 너무짧다)
*ex) "ㅇㅇㅇ 지수 1.2포인트 상승 출발", "ㅇㅇㅇ 지수 2.6포인트 하락 마감", "ㅁㅁㅁ 기업 영업이익 작년도 대비 12.7% 증가*

숫자가 너무 많이 들어간 경우도 있고 (숫자는 감성분석을 할 수 없다)
*ex) "지수선물 하락..286.30(-0.65p)"*

기업명이 들어간 기사가 너무 많이 올라오는 경우도 있는등
*ex) "ㅇㅇ제약, 작년 영업익 전년比 29.5%↑", "ㅁㅁ바이오, 상한가 진입.. +29.90% ↑"*

올라오는 뉴스의 퀄리티를 봤을때 '시황 및 전망' 카테고리가 가장 괜찮은듯 하여 선택했다. 여기도 문제가 없는건 아닌데 이중에서 그나마 제일 낫다.

3번째 예시로든 '기업명이 들어간 기사'를 제외시키기로 결정한 이유는, 논문의 본문에 '기업의 주가가 하락세로 돌아서면 기업에서는 홍보성 뉴스기사를 뿌리기 때문에(!) 각 종목에 대한 개별 기사의 경우는 정확도가 떨어진다' 라는 내용을 참고했기 때문이다. 

또한 [온라인 언급이 기업 성과에 미치는 영향 분석](http://bit.ly/2RKCLc3) 라는 논문에 따르면, "기업명이 사용된 뉴스의 경우, 기업의 실제 경영상태와 관련이 있는 뉴스인지 아니면 그냥 단순 언급인지 구분하지 않는한 정확도가 매우 떨어진다" 라고 한다. 그래서 뺐다.

즉, 예를들면 경제 칼럼니스트가 쓴 글처럼 내용이 길고 경제의 동향과 관련된 단어가 많이 쓰인 뉴스만 수집하는것에 중점을 뒀다.

이렇게 하여 수집된 뉴스데이터들

![collected_news_data]()

<br>
<br>

### 분석에 쓰일 단어 추출

###### 형태소 분석기

이제 수집한 뉴스를 형태소 분석기로 분석할 차례다. 

여기엔 여러가지 형태소 분석기 라이브러리들을 모아서 똑같은 인터페이스로 사용할수 있게 해주는 [KoNlpy](https://konlpy-ko.readthedocs.io/ko/v0.4.3/) 라는 라이브러리를 사용했다.

형태소 분석기에는 트위터, 꼬꼬마, 코모란 등등 여러가지가 있지만 여기저기 구글링을 해봐도 대체로 Mecab이 독보적이라 이걸 사용한다.

###### 필터링

Mecab에 13년치 모든 뉴스를 전부 집어넣어서 뽑아낸 단어셋(set)을 보니, 전체 뉴스 데이터(약 20만건)에서 딱 한건의 기사에만 나오거나 그에 준할만큼 적은 기사에만 등장하는 단어가 극단적으로 많았다. 때문에 분석에 쓰일 단어는 최소 10건 이상의 기사에 등장하는 단어로 한정한다.

한음절 단어와 명사를 제외한 모든 품사를 제거하였고 기업명(상장폐지포함)과 사람이름(ex: 기자이름)또한 제거했다. 

또한 '무단배포' '전재금지' '기자' 이런 단어들은 거의 모든 기사에서 나타나기 때문에 빈도수를 기준으로 한 백분위로 상/하위 3%의 단어는 쳐낸다.

이로써 분석에 쓰일 가장 기본적인 단어셋이 만들어졌다.

![wordset_basic_filtered]()

참고로 '전체 뉴스문서중 해당 단어가 나온 문서의 갯수' 를 DF(Document Frequency) 라고 부른다. 위의 '무단배포' '전제금지' 등의 단어는 DF가 과도하게 높을것이다. DF는 내가 만든건 아니고 검색기술과 관련된 업계에서 쓰이는 용어이다.

이런 용어중엔 TF(Term Frequency) 라는것도 있다. TF란 어떤 한개의 문서내에서 해당 단어가 얼마나 자주 등장하는지를 센 숫자이다. 예를들어 라면을 끓이는 방법에 대한 문서에서는 '라면'의 TF가 높을것은 자명하다. 

DF와 TF는 이제 곧 이어질 내용과 밀접한 연관이 있기 때문에 한번 짚고 넘어간다.

<br>
<br>

### 감성 사전 만들기

이제 각 단어의 감성이 긍정(이 단어가 쓰인 날엔 주가지수가 오른다)인지 또는 부정인지를 분석할 차례다. 단어별로 점수를 매기게 되는데 이를 '감성점수'라고 부르겠다. 점수가 클수록 강한 긍정이며 작을수록 강한 부정이다. 

각 단어별로 감성점수를 매겨놓은 집합을 '감성사전'이라고 부른다.

###### 감성점수 공식

단어의 감성점수를 매기는 방법은 논문마다 조금씩 다르긴 하지만 대체로 비슷한 모습을 띈다. 여러 논문들을 참고하여 만든 나의 단어별 감성점수 공식은 다음과 같다 

![senti_score]()

예를들어, 코스피의 등락이 30p, -10p, -5p 인 날짜에 단어가 쓰였다면 이 단어의 감성점수는 5가 된다. 공식의 핵심컨셉은 "상승장에 올라온 기사에 많이 쓰인 단어일수록 점수가 높다. 그리고 강한 상승장일수록 더 큰 점수를 준다." 이다. 하락장일 경우 역시 해당된다.

아쉽게도 위의 식은 TF에 대한 개념이 들어가 있지 않다. 예를들어 어떤날에 코스피가 50p나 올라서 상당한 강세장이라고 칠때, 이날 올라온 뉴스에 어떤 단어가 엄청 많이 사용되도 위 공식의 분모엔 +50밖에 안더해진다. 반대로 어떤 단어가 이날에 딱 한번밖에 안쓰였더라도, 일단 쓰였으면 이 역시 +50을 받는다.

TF에 해당하는 개념을 고려하지 않는점은 아쉽긴 하지만 일단은 진행하기로 결정했다. 만약 정말로 이 세상에 '뉴스와 주가지수간의 연관성'이 존재한다면, 이렇게 약간 덜 정확한 방식으로 얻어낸 결과물에서도 투박하게나마 경향성을 확인할수 있을것이라고 판단했기 때문이다. 일단 경향성을 확인했다면 본격적인 최적화는 나중에 천천히 해도 된다. 

여튼 이렇게 분석에 쓰일 감성사전이 만들어졌다. 

###### 감성사전으로 
